#!/usr/bin/env python3
"""
Simple Phoenix Integration Demo

This script demonstrates the Phoenix integration with a single request
to show how LLM observability works in practice.
"""

import requests
import time

# API base URL
BASE_URL = "http://localhost:8000"

def demo_phoenix_integration():
    """Demonstrate Phoenix integration with quiz-detail"""
    
    print("ğŸš€ Phoenix Integration Demo")
    print("=" * 50)
    print("This demo shows how Phoenix tracks LLM requests:")
    print("â€¢ Request tracing and performance")
    print("â€¢ Search result quality")
    print("â€¢ LLM enhancement metrics")
    print("â€¢ RAGAS evaluation tracking")
    print()
    
    # Test query
    query = "What is artificial intelligence?"
    print(f"ğŸ“ Query: {query}")
    print()
    
    # Start Phoenix server
    print("ğŸ”„ Starting Phoenix server...")
    try:
        response = requests.post(f"{BASE_URL}/phoenix/start")
        if response.status_code == 200:
            result = response.json()
            phoenix_url = result.get('phoenix_url')
            print(f"âœ… Phoenix server started!")
            print(f"ğŸŒ Access UI at: {phoenix_url}")
        else:
            print(f"âš ï¸ Phoenix server start failed: {response.status_code}")
    except Exception as e:
        print(f"âš ï¸ Phoenix server start failed: {e}")
    
    print()
    
    # Make request with Phoenix logging
    print("ğŸ”„ Making quiz-detail request with Phoenix logging...")
    
    payload = {
        "query": query,
        "use_llm": True,
        "enable_reranking": True,
        "reranking_strategy": "semantic_relevance",
        "enable_evaluation": True
    }
    
    try:
        start_time = time.time()
        response = requests.post(f"{BASE_URL}/quiz-detail", json=payload)
        total_time = time.time() - start_time
        
        if response.status_code == 200:
            result = response.json()
            print(f"âœ… Request successful! ({total_time:.3f}s)")
            
            # Show evaluation results
            if "evaluation" in result and result["evaluation"]:
                evaluation = result["evaluation"]
                if "error" not in evaluation:
                    metrics = evaluation.get("metrics", {})
                    print(f"\nğŸ“Š Quality Metrics:")
                    print(f"  ğŸ¯ Overall Score: {metrics.get('overall_score', 'N/A'):.3f}")
                    print(f"  ğŸ” Context Precision: {metrics.get('context_precision', 'N/A'):.3f}")
                    print(f"  ğŸ¤– Faithfulness: {metrics.get('faithfulness', 'N/A'):.3f}")
            
            print(f"\nğŸ” Phoenix Tracking:")
            print(f"  â€¢ Request traced with unique ID")
            print(f"  â€¢ Performance metrics logged")
            print(f"  â€¢ Quality evaluation recorded")
            print(f"  â€¢ Ready for analysis in Phoenix UI")
            
        else:
            print(f"âŒ Request failed: {response.status_code}")
            print(f"Error: {response.text}")
            
    except Exception as e:
        print(f"âŒ Request failed: {e}")
    
    print("\n" + "=" * 50)
    print("ğŸ¯ Demo Complete!")
    print("\nğŸ’¡ Next Steps:")
    print("  1. Open Phoenix UI in your browser")
    print("  2. View the trace for this request")
    print("  3. Analyze performance metrics")
    print("  4. Monitor quality trends")
    print("\nğŸš€ Your LLM observability is now active!")

if __name__ == "__main__":
    demo_phoenix_integration() 